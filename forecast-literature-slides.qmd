---
title: "Historical and Recent Trends in the Forecasting Literature"
subtitle: "Talk for the WARN-D Friday Science Meeting"
editor: visual
author: 
  - name: "Björn Siepe"
    orcid: 0000-0002-9558-4648
    email: bjoern.siepe@uni-marburg.de
    affiliations: 
      - name: "Psychological Methods Lab, University of Marburg"
        city: "Slides at bsiepe.github.io/talks"
    
date: "2024-02-16"
# from: markdown+emoji
format: 
  clean-revealjs:
    incremental: true
    embed-resources: true
    auto-stretch: true
    code-block-bg: true
    progress: true
bibliography: bibliography.bib
---

# Introduction {background-color="#40666e"}

## Why talk about this topic?

::: incremental
the secret reason:

![](figures/trojan_horse_meme.jpg){fig-align="center"}
:::

## Ok but really, why should you care? {.smaller}

::: fragment
"The quiet revolution of numerical weather prediction" [@bauerQuietRevolutionNumerical2015]:

![](figures/nature_fig1.png){.absolute top="150" left="100" width="750" height="450" cap-location="bottom"}
:::

# Forecasting Competitions {background-color="#40666e"}

## History

::: columns
::: {.column width="50%"}
### Critical statisticians 

-   We should be looking for the true model!

-   Maybe you do not know how to model with ARIMA...

-   

    > I suspect it is more likely to depend on the skill of the analyst … these authors are more at home with simple procedures than with Box-Jenkins. (Chatfield, based on @hyndmanBriefHistoryForecasting2020
:::

::: {.column width="50%"}
### Makridakis & Hibon (1979)

-   Our Empirical evidence disagrees <br/><br/>

-   Of course we do <br/> <br/>

-   

    > might be useful for Dr. Chatfield to read some of the psychological literature quoted in the main paper, and he can then learn a little more about biases
:::
:::

## M-Competitions {.smaller}

+-------------+----------+----------------+---------------------------------------------------------+
| Competition | Year     | N° Time Series | Insights/Novelty                                        |
+=============+==========+================+=========================================================+
| M1          | 1982     | 1001           | -   **Simple models** work well                         |
|             |          |                | -   **Combining forecasts** works well                  |
|             |          |                | -   Changed forecasting forever                         |
+-------------+----------+----------------+---------------------------------------------------------+
| M2          | 1993     | 29             | -   Not really relevant                                 |
+-------------+----------+----------------+---------------------------------------------------------+
| M3          | 2000     | 3003           | -   Somewhat simple models work well with modifications |
+-------------+----------+----------------+---------------------------------------------------------+
| M4          | 2020     | 100,000        | -   Combination of ML and Stats works well, pure ML not |
|             |          |                |                                                         |
|             |          |                | -   **Probabilistic Prediction**                        |
+-------------+----------+----------------+---------------------------------------------------------+
| M5          | 2021     | 42,000         | -   **Hierarchical Time Series**                        |
|             |          |                |                                                         |
|             |          |                | -   Ensembles + Pure ML works well                      |
+-------------+----------+----------------+---------------------------------------------------------+
| M6          | 2022     | 100            | -   ... to be continued                                 |
+-------------+----------+----------------+---------------------------------------------------------+

## Utility of Competitions {.smaller}

::: columns
::: {.column width="50%"}
### Advantages

-   Empirical evidence
-   Benchmarking
-   Methodological development and cumulative science [@fildes2004]
:::

::: {.column width="50%"}
### Issues

-   arbitrary conditions
-   vibe-based analysis of results [@koning2005]
-   Questionable generalizability [@fildes2004]
-   Reproducibility Issues? [@boylanReproducibilityForecastingResearch2015]
-   'Winner takes it all'? (see @stroblOneMethodFits2022
:::
:::

# Model Selection, Uncertainty & Combination {background-color="#40666e"}

## Model selection {.incremental .smaller}

::: fragment
"Typical" Workflow:

```{mermaid}
flowchart LR
    A(Use fancy models) --> B(Select best one)
    B --> C(Forecast)
    C --> D[Profit]
```
:::

::: fragment
Problem?
:::

::: fragment
-   Use simple models as benchmarks (see M-competitions, @makridakisStatisticalMachineLearning2018
:::

::: fragment
Selecting a single model:

-   ignores uncertainty about this selection [@kaplanQuantificationModelUncertainty2021]
-   tends to perform poorly in many forecasting settings [@chatfieldModelUncertaintyForecast1996]
:::

## Model combination {.smaller}

::: fragment
::: callout-tip
## 'Law' of forecast combination

'The results have been virtually unanimous: combining multiple forecasts leads to increased forecast accuracy. In many cases one can make dramatic performance improvements by simply averaging the forecasts.' [@clemen1989]
:::
:::

::: fragment
-   Many ways to combine forecasts [@wangForecastCombinations50year2023; @aastveitEvolutionForecastDensity2018]:
    -   Simple averaging
    -   Bayesian approaches [@yaoBayesianHierarchicalStacking2022; @yaoUsingStackingAverage2018; @dorieStanBARTCausal2022]
    -   Machine learning approaches (ensemble learning, trees, bagging, stacking, deep learning, etc.) [@petropoulosExploringSourcesUncertainty2018; @hastie2017]
:::

## Ensemble modeling {.smaller}

Two sources of uncertainty [@gneitingProbabilisticForecasting2014]:

![](figures/ensemble_prediction.png){fig-align="center"}

::: footnote
Met Office UK & @bauerQuietRevolutionNumerical2015
:::

## Relevance for psychology

-   Simple models as benchmarks [@makridakisStatisticalMachineLearning2018; @makridakis2022]
-   Model uncertainty often neglected in inferential and predictive modeling [@kaplanQuantificationModelUncertainty2021]

# Probabilistic Forecasting {background-color="#40666e"}

## Probabilistic forecasting methods

::: fragment
::: callout-tip
## Long history of probabilistic weather forecasting

'The probability of rain was much smaller than at other times.' (Dalton, 1793, based on @murphyEarlyHistoryProbability1998
:::
:::

::: fragment
![](figures/gneiting_uncertainty_fig1.jpeg){.absolute top="285" left="100" width="600" height="380"}
:::

## Relevance for psychology

::: fragment
-   Practically linked to decision theory in health settings, e.g., for JITAIs [@begoliNeedUncertaintyQuantification2019; @chen2021]
:::

::: fragment
![](figures/uncertainty_healthcare.png)
:::

# Combining Information {background-color="#40666e"}

## Extending mixed models {.smaller}

-   use of random effects gained attention in machine learning literature [@saldittGradientTreeBoosting2023; @sigristGaussianProcessBoosting2022; @kilianMixedEffectsMachine2023; @wortweinNeuralMixedEffects2023]

::: fragment
::: callout-tip
## Flexible Mixed Model

From

$$
\hat{y} = \underbrace{X\beta}_{\text{fixed effects}} + \underbrace{Z\upsilon}_{\text{random effects}} 
$$

to

$$
\hat{y} = \overbrace{ml_{fixed}(X)}^{\text{fixed effects}}+\overbrace{Z\upsilon}^{\text{random effects}}
$$

[@kilianMixedEffectsMachine2023]
:::
:::

## Relevance for Psychology {.smaller}

::: fragment
-   Improving on what we already have
-   Current combinations of 'idiographic' and 'nomethetic' forecasts: often ad-hoc
:::

::: fragment
![](figures/mixed-effects-ml.png)
:::

# Summary {background-color="#40666e"}

## Takeaways

1.  Forecasting competitions can lead to methodological improvement
2.  Model uncertainty and model combination are integral parts of forecasting
3.  Probabilistic forecasting is important for decision making
4.  Combining predictions is challenging, but lots of cool stuff in the making

::: fragment
[![giphy.com/goodfortunesonly](figures/giphy-future.gif){width="280"}](https://i.giphy.com/media/OJac5MRF6xJpqQAcR5/giphy.gif)
:::

## Me

::: columns
::: {.column width="40%"}
{{< fa house >}} [Feel](https://bsiepe.github.io/)

{{< fa brands twitter >}} [free](https://twitter.com/b_siepe)

{{< fa cloud >}} [to](https://bsky.app/profile/bsiepe.bsky.social)

{{< fa at >}} [contact](bjoern.siepe@uni-marburg.de)

{{< fa brands github >}} [me](https://github.com/bsiepe)

bjoern.siepe\@uni-marburg.de
:::

::: {.column width="60%"}
Slides at bsiepe.github.io/talks ![](figures/talks_qrcode.png)
:::
:::

## References {.smaller .scrollable}

::: {#refs}
:::

# Appendix

## Interesting packages

::: panel-tabset
### R

``` r
#|eval: false
library(forecast)    # OG package for time series forecasting
library(stan4bart)   # full-on probabilistic, multilevel, model combination
library(forecastHybrid)  # for model combination
library(Mcomp)       # data from m competitions
library(gpboost)     # xgboost-like, but probabilistic and multilevel
```

### Python

``` python
#|eval: false
import sktime   # for time series forecasting with sklearn-like interface
import skpro    # for probabilistic forecasting 
import gluonts  # for probabilistic forecasting, including deep-learning
import ngboost  # xgboost-like, but probabilistic!
import mapie    # model-agnostic, conformal prediction
import mlforecast # machine learning forecasting
```
:::

## 
